{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Iris Dataset Analysis\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step by step analysis of the renowned Iris dataset, offering a comprehensive guide for analyzing its various dimensions with Python. In addition, the content provided in this notebook aims to clearly explain each of the scripts created in Python for this analysis as well as the modules and functions used.     \n",
    "It takes users through a structured journey, starting with dataset loading and basic exploration, progressing to understanding variable types and modeling techniques. Subsequent sections delve into categorical data analysis and exploration of numerical variables via summary statistics and histograms. Then, further sections explore with scatterplots and heat map the relationship between the variables and ends with a program that prints the results of a correlation analysis carried out between each two variables of the dataset.       \n",
    "This systematic approach empowers users to grasp the dataset's intricacies and relationships, thereby aiding informed analysis and decision-making in relevant research or applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loading the Iris dataset:\n",
    "\n",
    "The program load_iris.py has been created to facilitate the loading of the Iris dataset into the programs summary.py, histogram.py, scatterplot.py, heatmap.py, and correlation.py. The function load_dataset is defined in load_iris.py and used in each of the mentioned programs to call in and run the script below in order to load the Iris dataset there. The script below can be broken down as follow:      \n",
    "\n",
    "<u>**Importing modules:**</u>  \n",
    "\n",
    "- The module OS is used to check if a file exists.\n",
    "- The module pandas is used to handle the Iris dataset in DataFrame format.   \n",
    "\n",
    "<u>**Fuctions definitions:**</u>   \n",
    "\n",
    "The function load_dataset(file_name) takes a filename as input and returns a DataFrame containing the Iris dataset. Within this function other functions are used:      \n",
    "- It first checks if the specified file exists using os.path.exists(file_name). If the file does not exist, it prints a message indicating the absence of the file and exits the program with quit(1).\n",
    "- If the file exists, it loads the dataset into a DataFrame using pd.read_csv(file_name, header=None), assuming the dataset is in CSV format without a header row.\n",
    "- Then, it defines column titles for the DataFrame using a predefined list column_title.\n",
    "- Finally, it sets the column titles as headers for the DataFrame and returns the resulting DataFrame. \n",
    "\n",
    "<u>**Main Execution:**</u>     \n",
    " \n",
    "- if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": This condition checks if the script is being run directly (not imported as a module).\n",
    "- load_dataset('iris.data'): Calls the load_dataset function with the filename 'iris.data'. If the script is executed directly, this line will execute the function and load the Iris dataset. If the file does not exist, it will print a warning message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Files: https://www.dataquest.io/blog/read-file-python/\n",
    "Check if file exists with OS: https://docs.python.org/3/library/os.path.html\n",
    "To understand what is the extension .data (iris.data) and how to work with it: https://www.askpython.com/python/examples/read-data-files-in-python#\n",
    "To add the title to the dataframe using pandas = https://sparkbyexamples.com/pandas/pandas-add-column-names-to-dataframe/\n",
    "Added header=None: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'os' to check if the file exists\n",
    "import os\n",
    "# Import 'pandas' for the data summary\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(file_name):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        # if file does not exist, then output a message\n",
    "        print(f'{file_name} does not exist. The \"iris.data\" file needs to be saved in the repository pands-project')\n",
    "        quit(1)\n",
    "    \n",
    "    # Load dataset \n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "\n",
    "    # Adding Column titles to the iris.data\n",
    "    # Defining the column titles\n",
    "    column_title = [\"Sepal Length (cm)\", \"Sepal Width (cm)\", \"Petal Length (cm)\", \"Petal Width (cm)\", \"Species\"]\n",
    "\n",
    "    # Setting column titles as headers to the dataframe\n",
    "    df.columns = column_title\n",
    "\n",
    "    # Load dataset and return it\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dataset ('iris.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary of the Iris dataset\n",
    "\n",
    "The program summary.py has been created with the goal of printing out a summary of the Iris dataset to a single text file called summary.txt. This program is loaded as a module into the main program analysis.py.\n",
    "\n",
    "\n",
    "### 2.1 Script:\n",
    "\n",
    "<u>**Import modules:**</u>     \n",
    "- pandas: used to load the dataset, summarize the data, and handling and manipulating the data structure.\n",
    "- io: it is used to create a string buffer for df.info().\n",
    "- load_iris: this custom module as described in section 1 of this iris_project.ipynb is imported to load the Iris dataset from a file, ensuringthe dataset is properly formatted and ready for analysis.\n",
    "- correlation: this is also a custom module and is responsible for performing correlation analysis on the dataset. This is imported into summary.py to print out the correlation analysis as section \n",
    "5 of summary.txt. Reference section 6 of iris_project.ipynb for more details on this module.\n",
    "- tabulate: this module is used to format the first 5 rows and last 5 rows of the dataset before writing them into the summary file. The printed out data is shown tabulated with borders to represent a table.\n",
    "\n",
    "<u>**Segments and functions used in this program:**</u>        \n",
    "\n",
    "The function \"create_summary(file_name)\" is defined in summary.py to generate a summary of each variable in the dataset, provide an overview of the first 5 rows and last 5 rows of the dataset, and save it all to a text file. This are the other functions used within create_summary:\n",
    "- load_dataset(file_name) is a function defined in the module load_iris.py and is used to load the Iris dataset from a file.\n",
    "- io.StringIO()Buffer creates a string buffer called variable_buffer. It's used to capture the output generated by the df.info() function, which provides information about the variables (columns), storing it for further processing or analysis.\n",
    "- Categorical Data snippet in Python aims to summarize categorical data in df. It starts with an empty dictionary called categorical_summary. Then, a 'for' loop is presented where for each column in the DataFrame that contains categorical data (identified using select_dtypes(include=['object'])), it counts the occurrences of each category, calculates the number of unique categories, and stores this information in the categorical_summary dictionary under the corresponding column name.\n",
    "- Continuous Data segment summarises continuous data within df, which is calculated using the describe() function. This function computes various descriptive statistics for each column containing numerical (continuous) data, such as count, mean, standard deviation, minimum, maximum, and quartile values, providing a comprehensive overview of the continuous variables in the dataset.\n",
    "- The segment 'Output to a single data file' writes a detailed summary of the Iris dataset, including an introduction, data type summary, categorical and continuous variable summaries, and correlation analysis, into a text file named \"summary.txt\". This is how it is done:\n",
    "    - \"with open('summary.txt', 'wt') as sf\" opens a file named \"summary.txt\" in write mode ('wt') and assigns it to the variable sf. The with statement ensures that the file is properly closed after its suite finishes, even if an exception is raised.\n",
    "    - \"sf.write('text')\" handles the writing of various summaries to the file \"summary.txt\". It sequentially writes the title, an introduction, an overview of the dataset's head and tail, a summary of data types, summaries for categorical and continuous variables, and finally, a summary of correlation analysis.\n",
    "    - The \"tabulate\" function formats the DataFrame outputs.\n",
    "    - \"variable_buffer.getvalue()\" retrieves the captured output of df.info() that was placed into the buffer \"variable_buffer\".\n",
    "    - \"create_correlation(file_name) computes correlation analysis for the dataset.\n",
    "\n",
    "<u>**Main Execution:**</u>\n",
    "- The script checks if it's being run as the main program (if __name__ == \"__main__\":) and generates a summary for the 'iris.data' dataset.\n",
    "    \n",
    "### 2.2 Summary.txt:\n",
    "\n",
    "The summary.txt outputs the following info from the Iris dataset:\n",
    "- 1. Introduction: Looking into the data - 5 first and last rows of the Iris dataset.\n",
    "- 2. Summary of the Data Types in Python - Displays the different Python data types used in Iris dataset, number of non-null counts and classify Python data types to categorical or continuous.\n",
    "- 3. Summary for Categorical Variables - for each categorical variable in the Iris dataset it prints out the count per value within the category.\n",
    "- 4. Summary for Continuous Variables - for each continuous variable within the dataset it returns the descriptive statistics, such as count of values, mean, standard deviation, minimum value, maximum value, and 25, 50 and 75 percentiles.\n",
    "- 5. Summary of Correlation Analysis - reference section 6 of this iris_project.ipynb.\n",
    "\n",
    "\n",
    "### 2.3 Outcomes from this analysis:\n",
    "\n",
    "The summary of the Iris dataset provides an in-depth analysis of its contents. It reveals a variety of data types, including measurements like sepal and petal sizes, as well as categorical data indicating the species of the iris flower. Through descriptive statistics, such as means, standard deviations, and quartiles, it illustrates the distribution of the continuous variables. Additionally, it highlights three distinct categories within the 'Species' variable: Iris-setosa, Iris-versicolor, and Iris-virginica, each with an equal representation of 50 occurrences. Furthermore, the correlation analysis of the Iris dataset shows strong positive correlations between petal length and width, moderate negative correlations between petal width and sepal width, and petal length and sepal width, while a weak negative correlation is observed between sepal length and width. Overall, this understanding facilitates informed decision-making and further analysis into the dataset. Creating visual representations, such as scatter plots and histograms, can aid to better understand the relationships between variables and the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data summary\n",
    "import io  # To use a string buffer for df.info()\n",
    "from load_iris import load_dataset  # To load the iris dataset \n",
    "from correlation import create_correlation  # To do the correlation analysis\n",
    "from tabulate import tabulate  # For table formatting\n",
    "\n",
    "def create_summary(file_name):\n",
    "    \"\"\"\n",
    "    Generates a summary of each variable in the dataset, provides an overview of the first 5 rows\n",
    "    and last 5 rows of the dataset, and saves it all to a text file.\n",
    "\n",
    "    Where the parameters are:\n",
    "        file_name: Name of the file containing the iris dataset.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = load_dataset(file_name)\n",
    "\n",
    "    # Summary of the variables\n",
    "\n",
    "    # Types of Variables of the Dataset\n",
    "    # Create a string buffer to capture the output\n",
    "    variable_buffer = io.StringIO()\n",
    "    # Capture the output of df.info()\n",
    "    df.info(buf=variable_buffer)\n",
    "\n",
    "    # Categorical Data\n",
    "    categorical_summary = {}\n",
    "    # Count: loop through each categorical variables\n",
    "    for column in df.select_dtypes(include=['object']):\n",
    "        # Count occurrences of each category\n",
    "        counts = df[column].value_counts()\n",
    "        # Get the number of unique categories\n",
    "        unique_categories = len(counts)\n",
    "        # Add summary to dictionary\n",
    "        categorical_summary[column] = {\n",
    "            'count': counts,\n",
    "            'unique_categories': unique_categories\n",
    "        }\n",
    "\n",
    "    # Continuous Data\n",
    "    # Calculate continuous summary\n",
    "    continuous_summary = df.describe()\n",
    "\n",
    "    # Output to a single text file\n",
    "    with open('summary.txt', 'wt') as sf:\n",
    "        # Write the title of the summary.txt file\n",
    "        sf.write(\"This is a summary of the Iris Dataset variables:\\n\\n\\n\")\n",
    "\n",
    "        # Write an overview of the first 5 rows of the dataset\n",
    "        sf.write(\"1. Introduction: Looking into the data\\n\\n\")\n",
    "        sf.write(\"1.1 Head of the Dataset:\\n\")\n",
    "        sf.write(\"This is a quick overview of the first 5 rows of the dataset.\\n\")\n",
    "        # Write the formatted DataFrame head to the summary file using tabulate\n",
    "        sf.write(tabulate(df.head(), headers='keys', tablefmt='grid') + \"\\n\\n\")\n",
    "\n",
    "        # Write an overview of the last 5 rows of the dataset\n",
    "        sf.write(\"1.2 Tail of the Dataset:\\n\")\n",
    "        sf.write(\"This is a quick overview of the last 5 rows of the dataset.\\n\")\n",
    "        # Write the formatted DataFrame tail to the summary file using tabulate\n",
    "        sf.write(tabulate(df.tail(), headers='keys', tablefmt='grid') + \"\\n\\n\\n\")\n",
    "\n",
    "        # Introduce the types of variables in the dataset\n",
    "        sf.write('2. Summary of the Data Types in Python:\\n\\n')\n",
    "        # Write the captured output of df.info() to the file\n",
    "        sf.write(variable_buffer.getvalue())\n",
    "        # Write the variable types and their corresponding types\n",
    "        sf.write('\\n2.1 Variables Types Classification based on Python Data Types: \\n\\nobject = Categorical Variable \\nfloat64 = Continuous Variable\\n\\n\\n')\n",
    "\n",
    "        # Write summaries for categorical variables\n",
    "        sf.write(\"3. Summary for Categorical Variables:\\n\\n\")\n",
    "        # Initialize the counter for variable numbering\n",
    "        counter = 1\n",
    "        # Iterate through each categorical variable and its summary\n",
    "        for variable, summary in categorical_summary.items():\n",
    "            # Write the variable name and its index\n",
    "            sf.write(f\"3.{counter} Variable: {variable}\\n\")\n",
    "            # Write the count of each category without headers\n",
    "            sf.write(summary['count'].to_string(header=False))\n",
    "            # Write the number of unique categories\n",
    "            sf.write(f\"\\n\\nUnique Categories: {summary['unique_categories']}\\n\\n\\n\")\n",
    "            # Increment the counter for the next variable\n",
    "            counter += 1 \n",
    "\n",
    "        # Write summary for continuous variables\n",
    "        sf.write(\"4. Summary for Continuous Variables:\\n\\n\")\n",
    "        # Initialize the counter for variable numbering\n",
    "        counter = 1\n",
    "        # Iterate through each continuous variable\n",
    "        for column in continuous_summary.columns:\n",
    "            # Write the variable name and its index\n",
    "            sf.write(f\"4.{counter} Variable: {column}\\n\")\n",
    "            # Iterate through each statistical measure for the variable\n",
    "            for statistic in continuous_summary.index:\n",
    "                # Write the statistic name and its corresponding value\n",
    "                sf.write(f\"{statistic.capitalize()}: {continuous_summary.loc[statistic, column]}\\n\")\n",
    "            # Add a newline after writing all statistics for a variable\n",
    "            sf.write(\"\\n\")\n",
    "            # Increment the counter for the next variable\n",
    "            counter += 1\n",
    "\n",
    "        # Generate a summary of correlation analysis and write it to summary.txt\n",
    "        sf.write(\"\\n5. Summary of Correlation Analysis:\\n\\n\")\n",
    "        # Call create_correlation function to compute correlation analysis\n",
    "        correlation_output = create_correlation(file_name)\n",
    "        # Write the correlation analysis output to the summary file\n",
    "        sf.write(correlation_output)\n",
    "\n",
    "# If this script is executed as the main program,\n",
    "# Generate a summary for the 'iris.data' dataset\n",
    "if __name__ == \"__main__\":\n",
    "    create_summary('iris.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Histogram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
